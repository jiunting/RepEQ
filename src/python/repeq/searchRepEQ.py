import glob
import matplotlib.pyplot as plt
import datetime
import obspy
from bs4 import BeautifulSoup
import pandas as pd
from obspy.taup import TauPyModel
#from obspy.clients.iris import Client
import numpy as np
from scipy import signal
import os,sys
import shutil

'''
1.search all the possible EQ pairs in the data directory
2.write p wave arrival time, stlo,stla,evla,evlo in the sac file
IN:
 eqpath:the directory path for seismic data, downloaded from obspy
 catalogpath:the catalog path that save the EQ info (generated by USGS Crawler, the input file for Massdownloader)
 repeq_dir:where do you want to save the output log files and figs
 filt_freq_HR:parameter for BP range
 p_wind:window to calculate CCC
OUT:
 output:repeq_dir/NAME.log, the logfile that save all CCC with any pairs
'''

def get_staloc(net_sta_key,n_date):
    xml_file=glob.glob(n_date+'/'+'stations/'+net_sta_key+'.xml')[0]
    tmpIN1=open(xml_file,'r').read()
    soup=BeautifulSoup(tmpIN1)
    stlon=float(soup.find_all('longitude' or 'Longitude')[0].text)
    stlat=float(soup.find_all('latitude' or 'Latitude')[0].text)
    return(stlon,stlat)

def get_traveltime(stlon,stlat,eqlon,eqlat,eqdep,model_name='iasp91'):
    #c=Client()
    #c=c.distaz(stalat=stlat,stalon=stlon,evtlat=eqlat,evtlon=eqlon)
    #dist,az,baz=obspy.geodetics.base.gps2dist_azimuth(lat1=evlat,lon1=evlon,lat2=stlat,lon2=stlon)
    dist_degree=obspy.geodetics.locations2degrees(lat1=eqlat,long1=eqlon,lat2=stlat,long2=stlon)
    model = TauPyModel(model=model_name)
    P=model.get_travel_times(source_depth_in_km=eqdep, distance_in_degree=dist_degree, phase_list=('P','p'), receiver_depth_in_km=0)
    S=model.get_travel_times(source_depth_in_km=eqdep, distance_in_degree=dist_degree, phase_list=('S','s'), receiver_depth_in_km=0)
    return(P[0].time,S[0].time,dist_degree)

def cal_CCF(data1,data2):
    #calculate normalize CCF, find max CCC, and lag idx
    tmpccf=signal.correlate(data1,data2,'full')
    auto1=signal.correlate(data1,data1,'full')
    auto2=signal.correlate(data2,data2,'full')
    tmpccf=tmpccf/np.sqrt(np.max(auto1)*np.max(auto2))
    maxCCC=np.max(tmpccf)
    lag=tmpccf.argmax()
    return(maxCCC,lag)


def cal_CCCscore(ndata,sav_ij_date,sav_CCC):
    CCCscore=np.zeros(ndata)
    for i in range(len(sav_ij_date)):
        CCCscore[sav_ij_date[i][0]]=CCCscore[sav_ij_date[i][0]]+sav_CCC[i]
        CCCscore[sav_ij_date[i][1]]=CCCscore[sav_ij_date[i][1]]+sav_CCC[i]
    return(CCCscore)



#-------------------------------------------------------------------------------#
eqpath='/Users/timlin/Documents/Project/EQrequest/Hawaii/Hawaii_ALL/' #where you put your waveform data (EQ folders)
#catalogpath='/Users/timlin/Documents/Project/EQrequest/Hawaii2018_EQ.dat' #EQ information
catalogpath='/Users/timlin/Documents/Project/EQrequest/Hawaii_ALL_M3.dat' #EQ information

repeq_dir='/Users/timlin/Documents/Project/EQrequest/Hawaii/reqEQ_log' #This is the output directory
A=pd.read_csv(catalogpath,header=None,sep=',',names=['time','eqlat','eqlon','eqdep','eqmag','x','x','x','x','x','x','x','x','x','x','x','x','x','x','x','x','x'],skiprows=0)
Cat_Date=pd.to_datetime(A['time'],format='%Y-%m-%dT%H:%M:%S.%fZ')#convert time to Datetime format

#filt_freq_HR=(0.5,2)
filt_freq_HR=(0.8,2) #Yu & Wen, (2012)
#filt_freq_HR=(1,4)
#filt_freq_HR=(0.03,0.1)
p_wind=(0,30)#seconds before and after theoritical P arrival

#-------------------------------------------------------------------------------#


EQfolders=glob.glob(eqpath+'*')
EQfolders.sort()

###Find the same net station name###
same_net_sta=[]
sav_info={} #dictionary that save the net_station time
for EQfolder in EQfolders:
    print('In:',EQfolder)
    sacs=glob.glob(EQfolder+'/waveforms/*.mseed')
    for sac in sacs:
        net=sac.split('/')[-1].split('.')[0]
        sta=sac.split('/')[-1].split('.')[1]
        print(net+'_'+sta)
        net_sta=net+'.'+sta
        try:
            sav_info[net_sta].append(EQfolder)
        except:
            sav_info[net_sta]=[EQfolder]
        if not(net_sta in same_net_sta):
            same_net_sta.append(net_sta)

######################################

same_net_sta.sort()
print(same_net_sta) #all the different stations in folder(eqpath)


#Do NOT change these flags!!
mkdirflag='N'
updateflag='N'
coarse_check='N'
logf_exist='N' #log file already exist?
if os.path.isdir(repeq_dir):
    print('directory already exist, overwrite the log files?')
    mkdirflag=input('overwrite the log file(N/Y)?')
    if mkdirflag=='Y':
        shutil.rmtree(repeq_dir)
        os.mkdir(repeq_dir)
    elif mkdirflag=='N':
        updateflag='N'
        print('The directory is already exist, update the existing log files?')
        updateflag=input('update the existing log files(N/Y)?')
        if updateflag=='N':
            print('Log files are already there, and you dont want to do anythong')
            sys.exit(2)
        elif updateflag=='Y':
            print('if you want to update the log file, but do NOT want to check into the log file, use coarse check')
            coarse_check=input('If the log file name already there, no need to check inside the file?(Y/N)')
            print('Continue to calculate')
        else:
            print('Only(N/Y) please')
            sys.exit(2)
    else:
        print('Only(Y/N) please')
        sys.exit(2)
else:
    os.mkdir(repeq_dir)


#sav_all_CCC=[]
###loop the net_sta and find the datetime
for net_sta_key in same_net_sta:
#for net_sta_key in ['BK.HUMO']:
    if coarse_check=='Y':
        if os.path.isfile(repeq_dir+'/'+net_sta_key+'.log'):
            print('Log file exist:',repeq_dir+'/'+net_sta_key+'.log','nothing to do...')
            continue
    print('------------looping stations----------------')
    print('Now running',net_sta_key)
    #for every keys
    if (mkdirflag=='N') & (updateflag=='Y'):
        if os.path.isfile(repeq_dir+'/'+net_sta_key+'.log'):
            logf_exist='Y'
            UPD1=open(repeq_dir+'/'+net_sta_key+'.log','r') #open the pre-existing file and check
            Chk_lines=UPD1.read()
            UPD1.close()
            OUT1=open(repeq_dir+'/'+net_sta_key+'.log','a')
        else:
            logf_exist='N'
            OUT1=open(repeq_dir+'/'+net_sta_key+'.log','w')
    else:
        logf_exist='N'
        OUT1=open(repeq_dir+'/'+net_sta_key+'.log','w')
    sav_Date=[]
    n=0
    pre_sampr=[]
    #sav_data_long=[]
    #sav_t_long=[]
    sav_data=[] #save all the same station data(different eq), loop them by Cn,2 to see if there is any repeat EQ
    sav_t=[]
    sav_Parrivl=[]
    sav_evlon=[]
    sav_evlat=[]
    sav_legend=[] #
    for n_date in sav_info[net_sta_key]:
        #for the same net_sta, different date
        YMDHms=n_date.split('/')[-1]
        Y=int(YMDHms[:4])
        M=int(YMDHms[4:6])
        D=int(YMDHms[6:8])
        H=int(YMDHms[8:10])
        m=int(YMDHms[10:12])
        s=int(YMDHms[12:14])
        print(Y,M,D,H,m,s)
        Date=datetime.datetime(Y,M,D,H,m,s) #event start time
        tmp_dt_all=Cat_Date-Date# see which event it is from the catalog
        junk_time=[] #find which one has the min time difference
        for tmp_dt in tmp_dt_all:
            junk_time.append(np.abs(tmp_dt.total_seconds()))
        idx_cat=np.where(junk_time==np.min(junk_time))[0][0] #now looking for this event, catalog may have duplicated, use only the first one!
        eqlon=float(A['eqlon'][idx_cat])
        eqlat=float(A['eqlat'][idx_cat])
        eqdep=float(A['eqdep'][idx_cat])
        if eqdep<0.0:
            continue
        sav_Date.append(Date)
        #read the data path by obspy
        data=obspy.read(n_date+'/waveforms/'+net_sta_key+'*.mseed')
        tmp_sampr=data[0].stats.sampling_rate #check sampling rate
        if pre_sampr==[]:
            pre_sampr=tmp_sampr #set the sampr for first station
        if tmp_sampr!=pre_sampr:
            print('Sampling rate inconsistent')
            continue
        #-------------get info------------
        stlon,stlat=get_staloc(net_sta_key,n_date)
        #get travel time
        tP,tS,GCARC=get_traveltime(stlon,stlat,eqlon,eqlat,eqdep,model_name='iasp91')
        sav_Parrivl.append(tP)   #P wave travel time
        #---------hang on, write the information in the sac file------
        #make sac a dictionary
        Otime=obspy.UTCDateTime(Date)-data[0].stats.starttime
        tP=tP+Otime
        tS=tS+Otime
        data[0].stats.update({'sac':{'t1':tP,'t2':tS,'o':Otime,'user1':tP-Otime,'user2':tS-Otime,'stlo':stlon,'stla':stlat,'evlo':eqlon,'evla':eqlat,'evdp':eqdep,'gcarc':GCARC}})
        data.write(n_date+'/waveforms/'+net_sta_key+'.sac',format='SAC')
        #--------ok, continue to process the data--------
        data.detrend('linear')
        data.taper(max_percentage=0.05)
        t=data[0].times()
        #bandpass filter
        data.filter('bandpass',freqmin=filt_freq_HR[0],freqmax=filt_freq_HR[1],corners=4,zerophase=True)
        y=data[0].data
        if np.isnan(y).any():
            print('detect Nan value in the data, continue to the next')
            continue
        #allign data by P-arrival
        idxt=np.where( (tP-p_wind[0]<=t) & (t<=tP+p_wind[1]) )[0] #find the index of P-waves
        Pwave_t=t[idxt]
        Pwave_y=y[idxt]/np.max(np.abs(y[idxt]))
        sav_data.append(Pwave_y) #sav_data: save Pwave data for all different date
        sav_t.append(Pwave_t)    #sav_t: save Pwave time
        #sav_data_long.append(y)
        #sav_t_long.append(t)
        #save earthquake location, for filtering
        sav_evlon.append(eqlon)
        sav_evlat.append(eqlat)
        sav_legend.append(n_date.split('/')[-1])
#        plt.plot(Pwave_t,Pwave_y)
#        plt.plot(stlon,stlat,'^')
#        plt.plot(eqlon,eqlat,'r*')
#        plt.text(stlon,stlat+0.08*n,net_sta_key+'_'+n_date.split('/')[-1][:4],rotation=0,fontsize=6)
        #clear the memory
        data.clear()
        n+=1
    #make cross correlation for any two date
    print('making CC for:%s, Total data:%s'%(net_sta_key,len(sav_data)))
    sav_ij_date=[]
    sav_CCC=[] #save the CCC for all ij_date
    for idate in range(len(sav_data)-1):
        for jdate in range(len(sav_data)):
            #print('CC for',idate,jdate)
            #check whether needs to calculate
            if (mkdirflag=='N') & (updateflag=='Y') & (logf_exist=='Y'):
                #log file exist and you must have already read it into a large string, right?
                if '%s-%s'%(sav_legend[idate],sav_legend[jdate]) in Chk_lines:
                    continue
            eqdist_degree=obspy.geodetics.locations2degrees(lat1=sav_evlat[idate],long1=sav_evlon[idate],lat2=sav_evlat[jdate],long2=sav_evlon[jdate])
            if (eqdist_degree>0.2):
                continue
            if jdate<=idate:
                continue
            CCC,lag=cal_CCF(sav_data[idate],sav_data[jdate])
            if np.isnan(CCC):
                continue
            sav_CCC.append(np.max(CCC))
            sav_ij_date.append((idate,jdate))
            #Output as a file
            OUT1.write('%s-%s %s %f\n'%(sav_legend[idate],sav_legend[jdate],net_sta_key,CCC))
            #Output figure, make figures for debug
            if CCC>=0.9:
                plt.figure()
                plt.plot(sav_t[idate],sav_data[idate],'k',linewidth=1)
                plt.plot(sav_t[jdate],sav_data[jdate],linewidth=1)
                plt.title(net_sta_key+' CC=%3.2f (unshifted)'%(CCC))
                plt.legend([sav_legend[idate],sav_legend[jdate]])
                plt.savefig(repeq_dir+'/'+net_sta_key+'.'+sav_legend[idate]+'_'+sav_legend[jdate]+'.png')
                plt.close() #close figure, don't want to show
            
    #plt.legend(use_legend)
    OUT1.close()

'''
plt.plot(sav_CCC)
plt.title('All CC calculations')
plt.ylabel('CC')
plt.show()
'''







#plt.show()

